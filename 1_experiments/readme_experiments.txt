This folder contains the LLM inferences (prompts and LLM responses) executed across 25 experiments.
one file per experiment document teh prompt and the LLM response, and was annotated with comments and observations during the qualitative assessment/aanlysis.
-----------------
Prompts were elaborated as follows:
•	Task Selection: sustainability-related decision-making tasks were identified that could benefit from using LLM-based AI support tools, such as energy optimization, plastics pollution, climate change mitigation strategies, environmental impact assessment via product lifecycle analysis. The task selection was mostly based on the identified relevant information sources.
•	Elaboration of an “initial” prompt, triggering an “LLM conversation”. An LLM conversation or “thread” is to be understood as a sequence of exchanges between the user and the model via a context window.
•	After the initial prompt, subsequent “main” prompts were formulated in function of the LLM initial response with the intention to refine, clarify, correct, guide or challenge the responses and so achieve the intended query results. Such prompts were labelled “main”, indicating that they are content related, and for which the user would expect an added value LLM response.
•	Also “subsidiary” prompts were formulated, which are unrelated to the study domain subject, but serve to trigger the LLM to provide clarity and explanations about why and how it responded the way it did. The objective herewith is to increase our understanding of the underlying mechanisms, rules and learning capabilities that LLMs apply to provide responses. 
•	Each prompt was described and characterized using a set of attributes, target metrics applicable to the prompt, and its coverage of decision analysis steps and UN SDGs. The prompt description and its applicable metrics were recorded in the Evaluation Matrix tool (see folder 2_...in this repository).
-----------------
Prompt execution: 
Each prompt was executed in conversational mode, with the user interacting via the context window of the AI tool as interface. The context window basic functionality usually allows textual input (the prompt question or task) and uploading files as supplementary information sources. The size limitations of the context windows in terms of maximum number of characters allowed were largely sufficient for performing the experiments. Additional information was uploaded as separate files, with file size limited to 1 Mb. This limitation hindered uploading some large files such as IPCC AR6 (2023) in their original format (.pdf), and had to be converted to a simpler file format (.txt). File format conversion implied loss of formatting and graphics but did not significantly impact LLM responses because all text was kept intact.
Model Outputs: responses generated by the models for each prompt were manually transferred from the LLM tool context window to an external editable word-processing file (cut-paste) for storage and further analysis.  Example screenshots are provided in annex 7.7

4.1	Exploratory experiments
This phase aimed at shedding light on what rules, principles, mechanisms and constraints were acquired by the reference LLM during pre-training, as a starting-point for in-context learning. We tested aspects described in section 3.3 and annex 7.5, such as ethics, fact checking and error generation, and observed and assessed how the LLM reacts to various domain related prompts and subsequent queries. LLM strengths and weaknesses became apparent in the inference responses. The value of understanding such pre-trained behavior lies in the fact that it constitutes the basis upon which in-context learning is built. Notably, any weakness already present in the model could impact the responses expected from in-context learning prompts, or vice versa, in-context learning prompts could influence (improve) responses that would be intrinsically weak or issue-prone. 
Experiments E01 to E13 were conducted during the exploration phase. In total 136 prompts were elaborated for this purpose, mainly zero-shot prompts, complemented with follow-up prompts on the same or new chat sessions, with and without the use of web search. 

4.2	Replication experiment
Experiment 14 was conducted as a replication experiment for testing LLM behavior and learning when using retrieval augmented generation (RAG) (Bubeck, 2023). The key motivation for this experiment was to address the research question about how well the LLM applies its general knowledge (AGI) to domain specific tasks, as compared to a specialized tool. 
This experimental phase was inspired by a research study by Vaghefi (2023), where GPT-4 is fine-tuned based on the authoritative IPCC AR6 (2023) and its annexes to answer climate change related prompts. To test the resulting “chatClimate” LLM, Vaghefi (2023) uses 13 main prompts for comparing GPT-4 responses with and without fine-tuning, and with or without web search use. 
For experiment E14 in our study, we used equally the IPCC AR6 (2023) to complement the prompts and used our reference model based on GPT-4o without fine-tuning to test AGI capabilities. Also, the initial prompts used for this experiment phase were those produced in Vaghefi (2023) complemented with subsequent main and subsidiary prompts (see section 3.4) to explore and challenge the LLM responses. In total 164 (8 n-shot and 156 zero-shot) prompts were formulated and executed for this phase, and replicating prompt variations combining web search use, RAG and LLM in-house knowledge. 

4.3	Enhancement experiments
Building upon the findings obtained by performing experiments E01 to E14, this phase went beyond and consisted in uncovering the impact of in-context learning on LLM responses. For this purpose, 25 N-shot prompts were defined and run on the reference LLM, and then the corresponding responses were assessed and rated, and the results recorded and analyzed.
Along the same lines as for Experiments E01 to E14, we tested the aspects described in section 3.3 and annex 7.5, such as use of web search, interpretability, ethics and error generation, and observed and assessed how the LLM reacts to various domain related prompts and subsequent queries. 
Experiments E15 to E25 were conducted during the exploration phase. A limited set of 25 N-shot prompts were elaborated for this purpose, including prompt variations to induce changes in LLM behavior. The number of examples provided in the prompts ranged between 3 and 6. All prompts were executed in new chat sessions to avoid same-chat contamination, and no constraints were placed on the model regarding the use of its own pre-trained knowledge or web retrieval features. 
